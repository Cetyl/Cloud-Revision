

# Setting Up a Databricks Cluster on AWS with IAM Roles

This guide walks you through the process of creating a Databricks account, configuring IAM roles in AWS, and setting up a Databricks cluster on AWS.

## Table of Contents
1. [Create an AWS Account](#step-1-create-an-aws-account)
2. [Create a Databricks Account](#step-2-create-a-databricks-account)
3. [Set Up IAM Roles in AWS](#step-3-set-up-iam-roles-in-aws)
4. [Create a Cluster in Databricks](#step-4-create-a-cluster-in-databricks)
5. [Work with Notebooks on Your Cluster](#step-5-work-with-notebooks-on-your-cluster)
6. [Terminate the Cluster](#step-6-terminate-the-cluster)

---

## Step 1: Create an AWS Account

To get started, you need an AWS account. If you don’t have one already, follow these steps:

1. Visit the [AWS sign-up page](https://aws.amazon.com/).
2. Click **"Create a Free Account"** and follow the instructions to set up an account.
3. Add billing information (AWS offers a free tier, so you will not incur charges unless you exceed the free-tier limits).
4. Verify your identity by phone and choose your support plan (the free Basic plan is sufficient for now).
5. Complete the setup, and log into the [AWS Management Console](https://aws.amazon.com/console/).

---

## Step 2: Create a Databricks Account

Once you have an AWS account, follow these steps to create your Databricks account:

1. Go to the [Databricks website](https://databricks.com/) and click **"Start a Free Trial"** or **"Get Started"**.
2. Select **"Start with AWS"** to sign up using your AWS account.
3. Log into your AWS account using your AWS credentials.
4. Provide details for your new Databricks workspace:
   - **Workspace Name**: Choose a name for your Databricks workspace.
   - **Region**: Choose an AWS region (e.g., US-East-1, US-West-2) where your Databricks instance will be created.
5. Once your workspace is provisioned, Databricks will provide a link to access your workspace. Use this link to log into your Databricks environment.

---

## Step 3: Set Up IAM Roles in AWS

Databricks requires IAM roles to interact with AWS services. Below are the IAM roles and policies commonly used:

### 1. **Databricks EC2 Role (Cluster Role)**

This IAM role grants Databricks clusters permissions to interact with AWS services like EC2, S3, CloudWatch logs, etc.

#### Create the Role:
1. Go to the **IAM Console** in AWS.
2. Click **"Create Role"**.
3. Select **"AWS Service"** as the trusted entity and choose **EC2**.
4. Click **Next: Permissions**.

#### Attach Policies:
- **AmazonS3FullAccess**: Grants access to S3 for reading and writing data.
- **AmazonEC2FullAccess**: Grants access to manage EC2 instances.
- **CloudWatchLogsFullAccess**: Allows Databricks to write logs to CloudWatch.
- **IAMReadOnlyAccess**: Allows Databricks to query IAM policies.
- **AmazonVPCFullAccess**: For managing EC2 instances in a VPC.

#### Review and Create the Role:
- Name the role `DatabricksEC2Role` and create it.

---

### 2. **Databricks S3 Access Role (Optional)**

If you want Databricks to have fine-grained access to S3, create a specific role for it.

#### Create the Role:
1. Go to the **IAM Console** and create a new role.
2. Select **Custom Trust Policy** and specify the trust relationship for Databricks.
3. Attach a policy like `AmazonS3FullAccess` or create a custom policy for more specific access.

Example of a custom policy for restricted S3 access:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::your-bucket-name/*"
    }
  ]
}
```

---

### 3. **Databricks Secrets Manager Role (Optional)**

If you're using **AWS Secrets Manager** to securely store sensitive information (e.g., AWS credentials), you need a role with access to Secrets Manager.

#### Create the Role:
1. Go to **IAM Console** and create a new role for Databricks to access AWS Secrets Manager.
2. Attach the policy `SecretsManagerReadWrite` for full access or a custom policy for restricted access.

Example policy for read-only access:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "secretsmanager:GetSecretValue",
        "secretsmanager:DescribeSecret"
      ],
      "Resource": "arn:aws:secretsmanager:region:account-id:secret:your-secret-name-*"
    }
  ]
}
```

---

## Step 4: Create a Cluster in Databricks

Now that you've set up the IAM roles, you can create a cluster in Databricks.

1. **Navigate to the Clusters Tab**:
   - In the left sidebar of Databricks, click **"Clusters"** under the **"Compute"** section.

2. **Create a New Cluster**:
   - Click on the **"Create Cluster"** button at the top-right.

3. **Configure the Cluster**:
   - **Cluster Name**: Choose a name for your cluster (e.g., `my-cluster`).
   - **Cluster Mode**: Select **"Standard"** or **"High Concurrency"** for multi-user workloads.
   - **Databricks Runtime Version**: Choose the latest Databricks runtime version (e.g., `7.x`).
   - **Node Type**: Select an AWS EC2 instance type (e.g., `m5.xlarge`, `r5.large`).
   - **Driver Type**: Select the instance type for the driver node.
   - **Worker Type**: Select the number of worker nodes.
   - **Autoscaling**: Enable autoscaling if you want the cluster to scale up or down automatically.
   - **Auto-Termination**: Set an auto-termination time (e.g., 30 minutes of inactivity).

4. **IAM Role Configuration**:
   - Under **Advanced Options**, specify the IAM role you created earlier (`DatabricksEC2Role`) in the **IAM Role** section.

5. **Create the Cluster**:
   - Click **"Create Cluster"** to provision your cluster. It may take a few minutes to become available.

---

## Step 5: Work with Notebooks on Your Cluster

Once your cluster is running, you can start creating and running code in Databricks notebooks:

1. **Create a Notebook**:
   - In the left sidebar, click **"Workspace"**.
   - Click **"Create"** > **"Notebook"**.
   - Select the programming language for your notebook (e.g., Python, Scala, SQL).
   - Choose the cluster you created from the **Cluster** dropdown.

2. **Run Code in Notebooks**:
   - Write and execute code (e.g., PySpark, SQL queries) and view the results directly in the notebook.

3. **Run Jobs**:
   - You can schedule notebooks to run as jobs. Go to the **"Jobs"** tab to set up job schedules.

---

## Step 6: Terminate the Cluster

When you’re done with the cluster, it’s important to terminate it to avoid incurring ongoing charges.

1. Go to the **"Clusters"** tab.
2. Select the cluster you want to terminate and click on its name.
3. Click the **"Terminate"** button to stop the cluster.

---

## Final Notes

- **Cost Considerations**: Running Databricks clusters on AWS incurs costs based on the EC2 instance type. Make sure to monitor your usage to avoid unexpected charges.
- **IAM Permissions**: Ensure IAM roles have the necessary permissions to interact with AWS services like S3, EC2, Secrets Manager, etc.
- **Scaling**: You can scale your cluster by adjusting the node types or enabling autoscaling.


```

